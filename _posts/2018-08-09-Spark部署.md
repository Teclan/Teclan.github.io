---
layout: post
title: Spark部署记录
date: 2018-08-09 15:18:12
categories: Spark
tags: [Spark]
---

# Spark部署记录


点击[此处](http://spark.apache.org/downloads.html)进入下载页面。
可选集成`hadoop`或不集成`hadoop`的Spar。

## 安装与启动

将下载的压缩包复制到指定目录，执行
```
tar -xf spark-2.2.0-bin-xxx.tgz
```

### 启动 master

进入解压后的目录，执行以下命令:

```
./sbin/start-master.sh
```
执行成功后进入 http://IP:8080/ 查看运行状态，看到`Workers`数量是0，
这是因为没有slave节点。

### 启动 slave 

进入解压后的目录，执行以下命令:

```
./sbin/start-slave.sh spark://host1:7077 # host1是机器域名或者IP
```
执行成功后进入 http://IP:8080/ 查看运行状态，看到`Workers`数量是1,说明有一
个从节点，可以别的机器上执行相同命令，将新的节点加入到集群。

## 关闭

分别执行以下命令:

```
./sbin/stop-master.sh
./sbin/stop-slave.sh
```

## 问题分析

### 错误信息
```
[hadoop@dellserver42 spark-2.2.0-bin-without-hadoop]$ ./sbin/start-master.sh 
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/BusinessCapability/spark-2.2.0-bin-without-hadoop/logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-dellserver42.yw.cn.out
failed to launch: nice -n 0 /home/hadoop/BusinessCapability/spark-2.2.0-bin-without-hadoop/bin/spark-class org.apache.spark.deploy.master.Master --host dellserver42.yw.cn --port 7077 --webui-port 8080
  错误: 无法初始化主类 org.apache.spark.deploy.master.Master
  原因: java.lang.NoClassDefFoundError: org/slf4j/Logger

```

### 解决办法

这是由于使用的是`without-hadoop`的`spark`,缺少`hadoop`的`classpath`导致，参考[Spark官方解释](
https://spark.apache.org/docs/latest/hadoop-provided.html)

```
Using Spark's "Hadoop Free" Build

Spark uses Hadoop client libraries for HDFS and YARN. Starting in version Spark 1.4, the project packages “Hadoop free” builds that lets you more easily connect a single Spark binary to any Hadoop version. To use these builds, you need to modify SPARK_DIST_CLASSPATH to include Hadoop’s package jars. The most convenient place to do this is by adding an entry in conf/spark-env.sh.

This page describes how to connect Spark to Hadoop for different types of distributions.
Apache Hadoop

For Apache distributions, you can use Hadoop’s ‘classpath’ command. For instance:

### in conf/spark-env.sh ###

# If 'hadoop' binary is on your PATH
export SPARK_DIST_CLASSPATH=$(hadoop classpath)

# With explicit path to 'hadoop' binary
export SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)

# Passing a Hadoop configuration directory
export SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)
```

在`$SPARK_HOME/conf/spark-env.sh` 中增加以上任意一种配置形式即可，例如:

```
export SCALA_HOME=/home/hadoop/soft/scala
export HADOOP_HOME=/home/hadoop/datacenter/hadoop

# 此处为增加的配置
export SPARK_DIST_CLASSPATH=$(hadoop classpath)

export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_JAR=/home/hadoop/spark/lib/spark-assembly-1.6.2-hadoop2.6.0.jar
export SPARK_MASTER_IP=xx.xx.xx.xx
```


